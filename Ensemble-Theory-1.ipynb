{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3W6cUBQ0S-E"
   },
   "source": [
    "## Classical statistical mechanics\n",
    "\n",
    "- Kardar Ch. 4\n",
    "- We skipped Ch. 1 because Thermo is a prerequisite for this class, and its principles will soon be rederived from a molecular viewpoint\n",
    "- We also skipped Ch. 3, though some of it will come back before we venture into quantum stat mech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw2MINxWIF3f"
   },
   "source": [
    "**Why do we need statistical mechanics?**\n",
    "\n",
    "* Thermal fluctuations cause physical systems to visit a multitude of microstates, rather than adopting well defined internal energy minima, \n",
    "* The set of \"typical\" microstates of a large system is the thermal equilibrium. Once kicked out of equilibrium, a large system quickly settles back into the same set of microstates\n",
    "* The description of large systems at thermal equilibrium is the focus of equilibrium statistical physics.\n",
    "* Phase Transitions and Critical Phenomena: Statistical mechanics is used to describe and predict how large systems transition between different states (e.g., liquid to gas), shedding light on universality and emergent behavior near critical points.\n",
    "* Quantum Many-Body Systems and Field Theory: Statistical mechanics provides fundamental tools for analyzing quantum systems at non-zero temperature, from electron gases in metals to high-energy particle collisions, enabling insights into phenomena like superconductivity and the quark-gluon plasma.\n",
    "* Statistical Physics of Live: Living systems consume energy to drive irreversible processes and stay away from thermal equilibrium. Nevertheless, they often show seemingly random fluctuations that can even dominate their behavior. It is active topic research how to extend statistical mechanics to describe such active systems far from equilibrium.\n",
    "* Complex Systems Beyond Physics: Methods from statistical mechanics underpin models in finance (e.g., econophysics), epidemiology (spread of diseases), network theory, and the dynamics of (machine) learning, where collective behavior emerges from many interacting units. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGMU9AklLj-G"
   },
   "source": [
    "### Closed systems -- microcanonical ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMKnKeDn2EVq"
   },
   "source": [
    "\n",
    "**Basic principle of statistical mechanics:** In an *isolated* system, all *allowed* *microstates* are equally likely.\n",
    "\n",
    "* An \"isolated\" system does not transfer energy, matter, volume or other conserved quantities outside its boundaries. A \"closed\" system can exchange energy, but not matter. \"Open\" systems typically exchange energy, matter and volume (e.g., biological systems like cells and organisms).\n",
    "* In classical physics (= all we care about for now), a \"microstate\" of a system of $N$ particles is a specific point $\\mu=\\{\\vec r, \\vec p\\}$ in $6N$-dimensional phase space. It describes the positions $\\vec r$ and momenta $\\vec p$ of all particles in the system.\n",
    "* All \"allowed\" microstates have the same energy and *generalized coordinates* $\\mathbf{x}$, which for a particle system typically consist of the volume of the container and the number of particles, i.e. $\\mathbf{x}=(V,N)$.\n",
    "* But the type of generalized coordinates we have to consider depends on the problem:\n",
    "    * If the system consists of different particle types, we need $\\mathbf{x}=(V,N_1,N_2, \\dots)$.\n",
    "    * If we consider magnets, the total magnetization $M$ is included.\n",
    "    * If we consider polymers, the length $L$ of the polymer is used.\n",
    "    * In all cases, generalized coordinates are always *extensive*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLSWV3dYcbnF"
   },
   "source": [
    "<img src=\"https://github.com/Hallatscheklab/SoftMatter22-23/blob/main/source/notebooks/L1/img/trail-of-bright-light-in-box-460688633-5966b3605f9b5816183316bb.webp?raw=1\" alt=\"systems\" name=\"systems\" class=\"bg-primary mb-1\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dl5NfUI57_K"
   },
   "source": [
    "Then, we can mathematize the basic principle by stating that the equilibrium distribution $P_{E,\\mathbf{x}}(\\mu)$ to observe a particular state $\\mu$ is given by\n",
    "\n",
    "$$P_{E,\\mathbf{x}}(\\mu)d\\mu=\\frac{\\delta(E-H(\\mu))}{\\Omega(E,\\mathbf{x})}d\\mu \\qquad (\\text{micro-canonical ensemble})$$\n",
    "\n",
    "where the normalization constant\n",
    "\n",
    "$$\\Omega(E,\\mathbf{x})\\equiv \\int \\delta(E-H(\\mu))\\;d\\mu $$ \n",
    "\n",
    "characterizes the total number of microstates {$\\mu$} with $H(\\mu)=E$ and $\\mathbf{x}_\\mu=\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for discrete set of micro states, e.g., when discussing Ising spins $\\sigma_i=\\pm 1$, replace the integral by a sum, $\\int d\\mu\\rightarrow \\sum_{\\mu}$, and the Dirac delta function by a Kronecker delta, $\\delta(E-H(\\mu))\\rightarrow \\delta_{E,H(\\mu)}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ergodicity \n",
    "\n",
    "One can imagine measuring expectation values $\\langle f(\\mu) \\rangle$ in two ways: 1. Follow a given system for a very long time, and determine the time average of $f$. 2. Determine the *ensemble* average of $f$ over a very large number of system states drawn from the distribution $P_{E,\\mathbf{x}}$. Our Basic Principles implies that, in the long-time limit, both results give the same answer. Mathematically,\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\langle f\\rangle_{E,\\mathbf{x}}=\\lim_{T\\to\\infty}\\frac 1T \\int_0^Tf(\\mu(t)) dt=\\int d\\mu P_{E,\\mathbf{x}}(\\mu) f(\\mu) \\text{\\quad (ergodic hypothesis)}\n",
    "\\end{align}\n",
    "\n",
    "The celebrated [ergodicity theorem](https://en.wikipedia.org/wiki/Ergodic_theory#:~:text=The%20concepts%20of%20ergodicity%20and,average%20over%20the%20entire%20space.) explains why this equality and, thus, our Basic Principle, is to be expected for typical systems undergoing volume conserving Hamiltonian dynamics (see later in the course). Macroscopic systems, such as ferromagnetic systems below the Curie temperature, can demonstrate ergodicity breaking, where they do not explore all possible states as predicted by the ergodic hypothesis. This results in phenomena like spontaneous magnetization, a form of spontaneous symmetry breaking. Similarly, complex disordered systems like spin glasses and conventional glasses, like window glasses, exhibit more intricate forms of ergodicity breaking. These systems can appear solid over short timescales, like seconds or hours, due to their positive shear modulus, but over very long periods, like millennia, they behave more like liquids, displaying multiple time scales and intermediate plateaus in behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "\n",
    "We can also argue for the micro-canonical ensemble to be the most unbiased (maximum entropy) distribution subject to the constraints $H(\\mu)=E$ and $\\mathbf{x}_\\mu=\\mathbf{x}$. Recall that the entropy of a discrete probability distribution is $S = -\\sum_i P_i \\ln(P_i)$, which is maximized for $P_i= \\text{const.} =\\Omega^{-1}(E,\\mathbf{x})$. In that case, $S=\\ln(\\Omega(E,\\mathbf{x}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historically, Boltzmann defined entropy to be the dimensionful quantity \n",
    "\n",
    "$$S\\equiv {\\rm k_B}\\ln(\\Omega)$$\n",
    "\n",
    "with ${\\rm k_B}\\sim 1.38×10^{−23} {\\rm J⋅K^{−1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "For a continuous phase space, $\\Omega$ is dimensionful. E.g. for an $N$-particle system, $\\Omega$ has units of phase space volume per energy, \n",
    "\n",
    "$$[\\Omega]= ([p][q])^{3N} [E]^{-1}\\;.$$\n",
    "\n",
    "To obtain a sensible definition of an *absolute* entropy, we should discretize phase space and energy to measure $\\Omega$ in some characteristic unit $[\\hat \\Omega]$, so that $S={\\rm k_B}\\ln(\\Omega/[\\hat \\Omega])$. That happens automatically in quantum mechanics, which provides natural units for phase space volume in terms of Planck's constant ($h^{3N}$) and ensures that energy levels are discretized.\n",
    "\n",
    "If we want to define an absolute entropy in classical statistical mechanics, Planck's constant is often borrowed from quantum mechanics. However, normally, classical statistical mechanics only cares about entropy differences, $S(E_1)-S(E_2)={\\rm k_B}\\ln(\\Omega_1/\\Omega_2)$, for which the discretization issue does not arise.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thermal equilibrium\n",
    "\n",
    "Consider an isolated system ${\\mu, E, \\boldsymbol{x}}$ consisting of two subsystem in contact so that they can exchange energy,\n",
    "\n",
    "![](figures/micro-can-compound-system.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu=\\mu_1 \\otimes \\mu_2$$\n",
    "\n",
    "$$E=E_1+E_2 +\\text{higher-order surface contribution}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B.:\n",
    "* E_1 and E_2 are extensive, meaning they grow linearly with the subsystem sizes.\n",
    "* The surface contribution is sub-extensive if the interactions are local.\n",
    "* Therefore, the surface contribution is negligible as subsystem sizes are sent to infinity (thermodynamic limit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of observing a particular state $\\mu$ of the compound system is, therefore, given by the microcanonical expression\n",
    "\n",
    "$$P_{E,\\mathbf{x}}(\\mu)=\\frac{\\delta(E-H(\\mu))}{\\Omega(E,\\mathbf{x})} $$\n",
    "\n",
    "where \n",
    "\n",
    "$$H(\\mu)= H_1(\\mu_1)+H_2(\\mu_2)+\\text{higher-order surface contribution}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the total number of available microstates of the compound system is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Omega(E, \\boldsymbol{x}) & =\\int d \\mu_{1} d \\mu_{2} \\delta(E-H(\\mu)) \\\\\n",
    "& =\\int d E_{1} \\Omega_{1}\\left(E_{1}, \\boldsymbol{x}_1\\right) \\Omega_{2}\\left(E-E_{1}, \\boldsymbol{x}_2\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "or equivalently\n",
    "$$\n",
    "e^{S(E) / k_{B}} =\\int d E_{1} e^{\\left[S_{1}\\left(E_{1}, \\boldsymbol{x}_1\\right)+S\\left(E-E_{1}, \\boldsymbol{x}_2\\right)\\right] / k_{B}}\n",
    "$$(X)\n",
    "(compound entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropies $S_{1}, S_{2}$ are extensive, so can be rewritten as\n",
    "\n",
    "$$\n",
    "S(E,\\boldsymbol{x})=N s_{1}\\left(\\frac{E_{1}}{N},\\frac{\\boldsymbol{x}_1}N\\right)+N s_{2}\\left(\\frac{E-E_{1}}{N},\\frac{\\boldsymbol{x}_2}N\\right)\n",
    "$$\n",
    "\n",
    "where $s_1$ and $s_2$ are entropies per particle, which approach a constant as we increase the system sizes.\n",
    "\n",
    "Thus, as $N \\rightarrow \\infty$, the integral in {eq}`X` is dominated by the maximum of the integrand, so that \n",
    "$$\n",
    "S(E,\\boldsymbol{x}) \\to S_{1}\\left(E_{1}^{*},\\boldsymbol{x}_1\\right)+S_{2}\\left(E_{2}^{*}=E-E_{1},\\boldsymbol{x}_2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the position of the maximum is obtained by extremizing w.r.t. $E_1$, resulting in \n",
    "\n",
    "$$\n",
    "\\left.\\frac{\\partial S_{1}(E_1^*)}{\\partial E_{1}}\\right|_{x_{1}}=\\left.\\frac{\\partial S_{2}(E_2^*=E-E_1^*)}{\\partial E_{2}}\\right|_{x_{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/thermalization-microcanonical.jpg\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employing the **thermodynamic definition of temperature** \n",
    "\n",
    "$$\\frac{1}{T}\\equiv\\left.\\frac{\\partial S}{\\partial E}\\right|_{\\boldsymbol{x}}\\;,$$ \n",
    "\n",
    "so that $\\frac{1}{T_1}\\equiv\\left.\\frac{\\partial S}{\\partial E}\\right|_{\\boldsymbol{x_1}}\\;,$ $\\frac{1}{T_2}\\equiv\\left.\\frac{\\partial S}{\\partial E}\\right|_{\\boldsymbol{x_2}}\\;,$ the maximum $E_1^{*}$ is set by\n",
    "\n",
    "$$\n",
    "\\frac{1}{T_{1}}=\\frac{1}{T_{2}}\\;.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "* $E_1$ (or $E_2=E-E_1$) adjusts itself so as to maximize $S$.\n",
    "* When $S$ is maximized, the temperatures of both compartments are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Law of Thermodynamics\n",
    "\n",
    "If we prepare an initial condition where $E_{1} \\neq E_{1}^{*}$, by ergodicity $E_{1} \\rightarrow E_{1}^{*}$ at long times because $E_{1}^{*}$ is *much more likely* (sketch). The entropy increases,\n",
    "\n",
    "$$\n",
    "\\Delta S=S_{1}\\left(E_{1}^{*}\\right)+S_{2}\\left(E_{2}^{*}\\right)-S_1\\left(E_{1}\\right)-S_2\\left(E_{2}\\right) \\geq 0\n",
    "$$\n",
    "\n",
    "which, to first order in $E_{1}^{*}-E_{1}=\\delta E_{1}$, implies\n",
    "\n",
    "$$\n",
    "\\left(\\frac{1}{T_{1}}-\\frac{1}{T_{2}}\\right) \\delta E_{1} \\geqslant 0 \\;,\n",
    "$$\n",
    "\n",
    "i.e., $T_2>T_1$ if $\\delta E_1>0$ and $T_1>T_2$ if $\\delta E_1<0$.\n",
    "\n",
    "Thus, **energy always flows from hot to cold.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stability:** Since the point $\\left(E_{1}^{*}, E_{2}^{*}=E-E_{1}^{*}\\right)$ is a maximum of $S\\left(E_{1}^{*}\\right)+S_{2}\\left(E-E_{1}^{*}\\right)=$ we must have\n",
    "\n",
    "$$\n",
    "\\left.\\frac{\\partial^{2} S_{1}}{\\partial E_1^{2}}\\right|_{x_{1}}+\\left.\\frac{\\partial^{2} S_{2}}{\\partial E_{2}^{2}}\\right|_{x_{2}} \\leq 0\n",
    "$$\n",
    "\n",
    "Suppose both systems are identical, $S_{1}=S_{2}$, \n",
    "\n",
    "$$\n",
    "\\left.\\Rightarrow \\partial_{E}^{2} S\\right|_{x} \\leq 0 \\Rightarrow \\textbf{S is concave}\n",
    "$$\n",
    "\n",
    "Also, since $\\left.\\partial_{E} S\\right|_\\mathbf{x}=T^{-1}$, we have $\\left.\\frac{\\partial E}{\\partial T}\\right|_\\mathbf{x} \\geq 0$.\n",
    "\n",
    "i.e., the **specific heat** $C_{x}\\equiv \\left.\\frac{\\partial E}{\\partial T}\\right|_\\mathbf{x}$ is **non-negative**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work and the First Law of Thermodynamics\n",
    "\n",
    "Thus far, we kept the generalized coordinates fixed and studied just the redistribution of energy. When we manipulate the generalized coordinates, we do work on the system.\n",
    "\n",
    "To analyze this situation, consider first changing $\\boldsymbol{x}\\to \\boldsymbol{x}+\\boldsymbol{\\delta x}$ in a slow, *reversible*, manner. (I.e. we can return to the initial state by reversing the externally applied forces $\\boldsymbol{J}$.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the energy will change, $E\\to E+\\boldsymbol{J} \\cdot \\boldsymbol{\\delta x}$, and so\n",
    "\n",
    "$$\\delta S=S(E+\\boldsymbol{J} \\cdot \\boldsymbol{\\delta x},\\boldsymbol{x}+\\boldsymbol{\\delta x})-S(E,\\boldsymbol{x})=\\left. \\frac{\\partial S}{\\partial E}\\right|_x(\\boldsymbol{J} \\cdot \\boldsymbol{\\delta x})+\\left.\\frac{\\partial S}{\\partial \\boldsymbol{x}}\\right|_E \\cdot \\boldsymbol{\\delta x}\\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a reversible dynamics, we must have the equal sign for all possible $\\boldsymbol{\\delta x}$, i.e.\n",
    "\n",
    "$$\n",
    "\\left.\\Rightarrow \\frac{\\partial S}{\\partial x_{i}}\\right|_{E, x_{j\\neq i}}=-\\frac{J_{i}}{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for a reversible transformation, the change in entropy becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Rightarrow \\delta S=\\frac{d E}{T}-\\frac{\\boldsymbol{J} \\cdot \\boldsymbol{\\delta x}}{T} $$\n",
    "\n",
    "which implies the **first law of thermodynamics**:\n",
    "\n",
    "$$\n",
    "\\Rightarrow dE=\\underbrace{T d S}_{d Q}+\\underbrace{\\boldsymbol{J} \\cdot \\boldsymbol{\\delta x}}_{dW}\n",
    "$$\n",
    "\n",
    "where the $dQ$ is the heat added to the system and $dW$ is the work done on the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oa8wy2MzbAGP",
    "tags": []
   },
   "source": [
    "### Energy exchange -- canonical ensemble\n",
    "\n",
    "Most soft matter systems are closed but not isolated: they trade energy and volume with the environment (a.k.a. *reservoir*, *bath*, *sorroundings*). What can we say about $P(\\nu_s)$ in those cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwOgjx6gcIou"
   },
   "source": [
    "The combination of System (index s) and Reservoir (index r) forms an *isolated* system with fixed $E$, $V$, $N$. We can therefore apply our \"Basic Principle\" to conclude\n",
    "\n",
    "* All states with $E_s+E_r=E$ are equally likely.\n",
    "* To reach a high energy state, s has to \"steal\" energy from b.\n",
    "What's the probability $P(\\nu_s)$ of system state $\\nu_s$ irrespective of the state of the bath?\n",
    "\n",
    "$$P(\\nu_s)=\\sum_{\\nu_r} P(\\nu_s,\\nu_r)=\\frac{\\Omega_r(E-E_s)}{\\Omega(E,V,N)}$$\n",
    "\n",
    "$$\\boxed{\\rightarrow P(\\nu_s)\\propto \\Omega_r(E-E_s)}$$\n",
    "\n",
    "Note that **the multiplicity of the bath alone controls the state of the system.** I.e. the more states are available to the bath the more likely a given microstate of the system is. Energy exchange is important only in sofar as it modifies the number of states available to the bath and the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCe_sGpfhKHx"
   },
   "source": [
    "The multiplicity of microstates grows exponentially with its parameters. This can be most easily demonstrated for particle numbers (see example below) but generally holds also for energy and volume. Therefore, it makes sense to introduce the logarithm of the multiplicity, as Ludwig Boltzmann did when introduced $ S\\equiv k_\\mathrm{B} \\ln \\Omega \\qquad k_\\mathrm{B}=1.38064852 × 10^{-23} {\\rm J/ K}$ (see above).\n",
    "\n",
    "Using the Boltzmann entropy, we can write \n",
    "\n",
    "$$P(\\nu_s)\\propto e^{k_\\mathrm{B}^{-1}S_r(E-E_s)}.$$\n",
    "\n",
    "When the bath is very big (thermodynamic limit), we can Taylor expand the exponent,\n",
    "\n",
    "$$ S_s(E-E_s)=S_r(E)+\\left(\\frac{\\partial S}{\\partial E}\\right)_{V,N}(-E_s)+\\cdots $$\n",
    "\n",
    "Inserting above yields \n",
    "\n",
    "$$\\boxed{P(\\nu_s)\\propto \\exp\\left[\\text{const.}-k_B^{-1}\\left(\\frac{\\partial S_r}{\\partial E}\\right)_{V, N}E_s\\right]\\propto e^{-b E_s}\\;,\\qquad \\text{where } b\\equiv k_B^{-1}\\left(\\frac{\\partial S_r}{\\partial E}\\right)_{V, N}\\;.} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7P92EfznDKX"
   },
   "source": [
    "The factor $e^{-b E_s}$ is the **Boltzmann factor**, if we identify $b$ and $\\beta_r\\equiv (k_\\mathrm{B} T_r)^{-1}$. \n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\\beta_r\\equiv \\left(\\frac{\\partial \\ln \\Omega_r}{\\partial E}\\right)_{V, N}=\\frac 1{k_\\mathrm{B}} \\left(\\frac{\\partial S_r}{\\partial E}\\right)_{V, N}.$$\n",
    "\n",
    "Equivalently, \n",
    "\n",
    "$$\\boxed{T_r\\equiv \\left(\\frac{\\partial S_r}{\\partial E}\\right)_{V, N}^{-1}}$$\n",
    "\n",
    "is the  thermodynamic definition of the temperature of the reservoir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-3pU6kUSqqw"
   },
   "source": [
    "**Implication: The ratio of probabilities of two microstates is given by the ratio of their Boltzmann factors**\n",
    "\n",
    "$$\\frac{P(\\nu_s^{(1)})}{P(\\nu_s^{(2)})}=e^{-\\frac{E_s^{(2)}-E_s^{(1)}}{k_\\mathrm{B} T_r}}=e^{-\\frac{\\Delta E}{k_\\mathrm{B} T_r}}$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Example: Barometric height formula.**\n",
    "    \n",
    "Consider a constant gravitational force $F=-mg$ along the $z$-direction such that the potential energy is given by $E=mgz+E_0$ where $E_0$ is the energy at ground level. For simplicity, we set $E_0=0$.\n",
    "\n",
    "The potential energy difference due to a height difference $\\Delta z$ is given by $\\Delta E = m g \\Delta z$. Therefore, the ratio of the probabilities to find a particle at a given position compared to a position elevated by a height $\\Delta z$ is given by $$\\frac{P(\\nu_s^{(1)})}{P(\\nu_s^{(2)})}=\\exp\\left(-\\frac{\\Delta z}{\\lambda}\\right), \\qquad \\text{in terms of the ``scale height''} \\lambda\\equiv \\frac{k_B T}{m g}\\approx 8 \\text{km}.$$ Thus, increasing your elevation by 8 km approximately leads to an e-fold reduction of the air density. (\"approximately\", because (i) the temperature changes with height, (ii) molecules interact and (ii) the atmosphere is not in equilibrium). \n",
    "\n",
    "So far, we just cared about ratios of probabilities. In general it is much harder to compute absolute probabilities, which requires computing a normalization constant -- the partition function. In the present one-dimensional case, however, we can do it easily: The probability $p(z)$ for finding the particle at position $z$ is given by \n",
    "$p(z)=\\frac{1}{\\lambda}\\exp\\left( -\\frac{z}{\\lambda}\\right)$, which is properly normalized, $\\int_0^\\infty p(z)=1$.\n",
    "\n",
    "    \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "    \n",
    "**Example: Two-level-system**\n",
    "\n",
    "As another frequent application of Boltzmann’s law are state populations of two state systems, as we find them frequently in physics, e.g., for spin systems. Such two level spin systems are, for example, very important for nuclear magnetic resonance (NMR), which is an important tool to study the structure and dynamics of soft matter.\n",
    "\n",
    "![Two Level](figures/two_level.png)\n",
    "    \n",
    "Consider the image above, where a single energy level at zero magnetic field ($B=0$) splits into two energy levels due to the interaction of a proton spin (red arrow) with the external magnetic field.\n",
    "    \n",
    "The magnetic moment of the proton spin may take two expectation values in the magnetic field, which are characterized by the magnetic quantum number $m_Z=\\pm 1/2$. The magnetic moment projected along the magnetic field direction is then \n",
    "    \n",
    "$$\\mu_Z=\\gamma m_Z \\hbar$$\n",
    "\n",
    "and the energy of the states\n",
    "    \n",
    "$$E(m_Z)=-\\mu_Z B=-\\gamma m_Z \\hbar B$$\n",
    "\n",
    "with $\\gamma=2.675222005\\times 10^{8}\\; \\mathrm{s}^{-1} \\mathrm{T}^{-1}$ being the gyromagnetic ratio of the proton.\n",
    "The energy difference for a nonzero magnetic field is therefore given by\n",
    "    \n",
    "$$\\Delta E=\\gamma \\hbar B=\\hbar \\omega_\\mathrm{L}$$\n",
    "    \n",
    "which results for a magnetic field of $B=1\\; \\mathrm{T}$ in $\\Delta E\\approx 1.76\\times 10^{-7}\\;{\\rm eV}$ or a Larmor frequency of $\\omega_\\mathrm{L}=42\\; {\\rm MHz}$. This energy difference is almost negligible as compared to the thermal energy at room temperature $k_\\mathrm{B}T=2.6\\times 10^{-2}\\;{\\rm eV}$. Yet, this small energy difference is used to give the contrast in NMR and related techniques such as MRI.\n",
    "    \n",
    "Using the Boltzmann distribution we can now calculate the ratio of the population of spins in the lower or excited state\n",
    "    \n",
    "$$\\frac{N_{-\\frac{1}{2}}}{N_{+\\frac{1}{2}}}=\\exp\\left (-\\frac{\\Delta E}{k_\\mathrm{B} T} \\right )$$\n",
    "    \n",
    "which is very close to one: \n",
    "\n",
    "$$\\frac{N_{-\\frac{1}{2}}}{N_{+\\frac{1}{2}}}=0.99999332.$$\n",
    "    \n",
    "If you consider now a volume of $V=1\\;{\\rm {\\mu m}}^3$ water, then you would roughly have about $N=6.7\\times 10^{19}$ protons. This then means that the excess number of protons in the excited state is just $N_{+\\frac{1}{2}}-N_{-\\frac{1}{2}}=4.5\\times 10^{12}$, which is extremely low. Thus, to detect something in NMR or MRI, a certain number of protons in the volume is required.    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
