{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0833fe-3ba6-47e6-b487-d39a5eed4514",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Probability, energy and entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f34d7a-a7c4-46c2-98e2-c277c3d30e2d",
   "metadata": {},
   "source": [
    "Agenda:\n",
    "- microstates vs macrostates\n",
    "- emerging collective behavior\n",
    "- universality (CLT)\n",
    "- entropy vs energy\n",
    "- probability recap \n",
    "\n",
    "Reading:\n",
    "- Arovas, Ch. 1\n",
    "- Kardar, Ch. 2\n",
    "\n",
    "## Coin flipping  \n",
    "\n",
    "We will use the simple example of repeatedly flipping a biased coin as a warm-up excercise to illustrate key concepts in statistical physics. \n",
    "\n",
    "Coin flipping arises in many context:\n",
    "- spatial configuration of magnetic moments: 1D Ising spin chain \n",
    "- time series of left- or right steps in a random walk or a polymer (1D freely jointed chain)\n",
    "\n",
    "### Setup\n",
    "\n",
    "We assume that the coin is flipped $N$ times (~\"volume\" of spin chain) and describe the outcome of coin flip $i$ as $\\sigma_{i} \\in\\{-1,+1\\}$.\n",
    "\n",
    "For a single coin flip, the probabilities of heads and tails are \n",
    "\n",
    "$$P\\left(\\sigma_{i}\\right)=\\left\\{\\begin{array}{l}p \\;, \\quad\\sigma=1 \\\\ q=1-p\\;, \\quad \\sigma=-1\\end{array}\\right.\\;,$$\n",
    "\n",
    "which can also be concisely expressed as $P\\left(\\sigma_{i}\\right)=p \\cdot \\delta_{\\sigma_i 1}+q \\delta_{\\sigma_i 0}$, which is sometimes useful.\n",
    "\n",
    "Since coin flips are uncorrelated (by assumption), the probability of observing a particular configuration is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P\\left(\\sigma_{1}, \\sigma_{2}, \\ldots\\right) & = \\prod_{j} P\\left(\\sigma_{j}\\right) \\\\\n",
    "& \\text { (uncorrelated spins) }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In principle, everything one can say about the statistics of coin flips follows from this expression.  \n",
    "\n",
    "*Question:* Compare\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& A=\\{++--+++-+-+++---+++-+-++-+\\} \\\\\n",
    "& B=\\{++++++++++++++++++++++++++\\} \\\\\n",
    "& C=\\{+-+-+-+-+-+-+-+-+-+-+-+-+-\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If $p=q=\\frac{1}{2}, P(A)=P(B)=P(C)$. So, why does $B$ (and $C$?) look exceptional?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25736a4a-196a-4571-9e11-79668893a5e4",
   "metadata": {},
   "source": [
    "### Coarse-graining\n",
    "\n",
    "In the realm of statistical physics, the concept of **coarse-graining** plays a pivotal role in understanding the transition from microscopic to macroscopic descriptions. The entire configuration of a system, denoted by $\\{\\sigma\\} = \\left\\{\\sigma_{1}, \\sigma_{2}, \\ldots\\right\\}$, represents the **micro-state**. The set of all micro-states encompasses all possible configurations at the most fundamental level.\n",
    "\n",
    "\n",
    "**However, we are rarely concerned with the exact details of individual microstates. Instead, we focus on features common to all typical microstates.** In most cases, the specific microstate of a system—such as the precise arrangement of spins in a solid or the exact positions and momenta of molecules in a glass of water—offers little insight into its macroscopic behavior. Questions like “What happens to water when it is heated?” or “How does a balloon respond to pressure?” relate to emergent properties derived from the shared characteristics of the vast number of possible microstates in a macroscopic system (on the order of Avogadro’s number).\n",
    "\n",
    "For instance, when observing a sequence of coin flips represented as 1’s and 0’s, we are generally uninterested in the precise order of outcomes. Instead, we notice patterns, such as the balance between 1’s and 0’s. This leads us to consider quantities like the total number of heads $N_+$, tails $N_-$, or their difference,\n",
    "\n",
    "$$X \\equiv \\sum_{j=1}^{N} \\sigma_{j}=N_+ - N_-\\;.$$\n",
    "\n",
    "Similarly, in a magnet, individual spins are difficult to measure, but we can focus on macroscopic quantities like the magnetization, which is also expressed as $X = N_+ - N_-$, the net difference between up and down spins.\n",
    "\n",
    "Since mapping ${\\sigma} \\rightarrow X$ reduces detailed information to a single summary quantity, $X$ is known as a macroscopic observable. Fixing $X$ defines a macrostate, which captures the system’s behavior at a higher level of abstraction. (The associated loss of information is a key quantity central to both information theory and statistical mechanics. More on that later.)\n",
    "\n",
    "**A primary goal in statistical physics is to transition from the probability distribution of microstates, $P({\\sigma})$, to the probability distribution of macrostates, $P(X)$.** Understanding this transformation is central to the discipline. To explore this, consider a coin-flipping example.\n",
    "\n",
    "The probability of observing a macrostate $X$ depends on two factors: the number of microstates $\\binom{N}{N_+}$ consistent with $X$, and the probability $p^{N_+}q^{N_-}$ of any specific microstate. This yields the binomial distribution:\n",
    "\n",
    "$$\n",
    "P(X)=P(N_+,N_+)=\\underbrace{\\frac{N!}{N_{+}! N_{-}!}}_{\\equiv e^{S(X)}} \\quad \\underbrace{p^{N_{+}}q^{N_{-}}}_{\\equiv e^{-E(X)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(X)= e^{S(X)}e^{-E(X)}\n",
    "$$\n",
    "\n",
    "Here, we introduced the quantities $S(X)$ as the **log of the number of microstates** and $E(X)$ as the negative of the **log-probability of a single such microstate**. Up to pre-factors, these quantities are **entropy** and **energy** in statistical physics. They always compete with one another ....  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& S=\\ln (N !)-\\ln \\left(N_{+} !\\right)-\\ln \\left(N_{-} !\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we invoke Stirling's approximation, which is given by\n",
    "\\begin{aligned}\n",
    "\\ln N !=N(\\ln N-1)+O(\\ln N)\n",
    "\\end{aligned}\n",
    "and can be roughly derived as $\\ln N! = \\sum_{j=1}^N \\ln j \\approx \\int_1^N dx \\ln x = N(\\ln N - 1) + 1$, which successfully captures the leading dependence.\n",
    "\n",
    "Using Stirling's approximation, we find for the entropy\n",
    "\n",
    "$$\n",
    "S \\approx-N\\left(\\frac{1+x}{2} \\ln \\left(\\frac{1+x}{2}\\right)+\\frac{1-x}{2} \\ln \\left(\\frac{1-x}{2}\\right)\\right)\\;,\n",
    "$$ (entropy-coin-flipping)\n",
    "\n",
    "where we introduced the specific magnetization $x=X/N$. \n",
    "\n",
    "The energy can be written as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(x) & =-\\ln \\left(p^{N_{+}} q^{N_{-}}\\right)=-N_{+} \\ln (p)-N_{-} \\ln (q) \\\\\n",
    "& =-N\\left[\\frac{1+x}{2} \\ln p+\\frac{1-x}{2} \\ln q\\right] \\\\\n",
    "& =-\\frac{N}{2}\\left[\\ln (p q)+x \\ln \\frac{p}{q}\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00335cbf-6057-474b-a88f-93145d20e238",
   "metadata": {},
   "source": [
    "```{note}\n",
    "For large $N$, both entropy $S\\propto N$ and energy $E\\propto N$ have a very simple, linear dependence on $N$. Macroscopic observables that grow linearly with the system size are called **extensive**. These are in contrast to **intensive** quanitites, which approach a constant in the thermodynamic limit of large systems, for example $x=X/N$ which is bounded between -1 and 1. \n",
    "\n",
    "**Throughout this course, we use capital/lower case letters to indicate extensive/intensive quantities.**\n",
    "```\n",
    "\n",
    "\n",
    "The following plots illustrate the competition between energy and entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4357996-cb42-43dc-afb2-eb6e158f71e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function s(x)\n",
    "def s(x):\n",
    "    return -(1+x)/2 * np.log((1+x)/2) - (1-x)/2 * np.log((1-x)/2)\n",
    "\n",
    "# Create an array of x values from -1 to 1\n",
    "x = np.linspace(-1, 1, 400)\n",
    "\n",
    "# Avoid division by zero and log of zero by slightly adjusting the range\n",
    "x = np.clip(x, -0.999999, 0.999999)\n",
    "\n",
    "# Compute s(x) for these x values\n",
    "y = s(x)\n",
    "\n",
    "\n",
    "# Define the function e(x) with p = 0.7 and q = 0.3\n",
    "p = 0.7\n",
    "q = 0.3\n",
    "\n",
    "def e(x):\n",
    "    return -0.5 * (np.log(p * q) + x * np.log(p / q))\n",
    "\n",
    "# Compute e(x) for the same x values\n",
    "y_e = e(x)\n",
    "\n",
    "# Compute the sum of s(x) and e(x)\n",
    "y_sum = y - y_e\n",
    "\n",
    "# Create the plot with s(x), e(x), and their difference\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, label=r'$s(x)=-\\left(\\frac{1+x}{2} \\ln \\left(\\frac{1+x}{2}\\right)+\\frac{1-x}{2} \\ln \\left(\\frac{1-x}{2}\\right)\\right)$', linestyle='dotted')\n",
    "plt.plot(x, y_e, label=r'$e(x)=-\\frac{1}{2}\\left[\\ln (pq)+x \\ln \\frac{p}{q}\\right]$', linestyle='dashed')\n",
    "plt.plot(x, y_sum, label=r'Difference of $s(x)$ and $e(x)$, which is $\\ln P(x)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Plot of s(x), e(x), and their difference; p='+str(p)+' q='+str(q))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33caf374-9c7a-47aa-90b7-ba83412095d5",
   "metadata": {},
   "source": [
    "Notice:\n",
    "- $s(x)$ and $e(x)$ are the entropy and energy per coin, respectively (see legend).\n",
    "- The entropy $s(x)$ has a maximum at the neutral point $x = 0$ and vanishes at the boundaries (all spin up / down)/\n",
    "- \"Low energy\" states have higher probability.\n",
    "- The competition between $e(x)$ and $s(x)$ leads to a maximum in their difference, $s(x)-e(x)$.\n",
    "- Later, we will rather look at the negative, $e(x)-s(x)=:f(x)$, which we call a free energy. It is generally a convex function and has a *minimum* at the most likely value of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933608fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09ea2498-8b0e-470e-8087-198bd74bc51a",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The depicted maximum in $-f(x)$ does not look very impressive. But, since we have\n",
    "\n",
    "$$P(x)=e^{-N f(x)},$$\n",
    "\n",
    "a so-called (large deviation principle)[https://en.wikipedia.org/wiki/Large_deviations_theory], we will get a pronounced maximum at $x=p-q$ (the minimum of $f(x)$) for even modestly large $N$. Fully deterministic behavior $x \\to p-q$ emerges in the thermodynamic limit, $N\\to \\infty$. The most likely value of $x$ coincides with the expectation value $\\langle x \\rangle = p-q$ (see below).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218f049-ca49-417d-af00-42f2cff4af8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can explore the effects of finite $N$ by Taylor expanding $S(x)-E(x)$ to $O(\\Delta x^2)\\equiv O\\left[(x-\\langle x\\rangle)^{2}\\right]$, we obtain to leading order a Gaussian,\n",
    "\n",
    "$$\n",
    "P(x) \\approx C e^{-\\left[\\frac{(\\Delta x)^{2}}{2 \\langle \\Delta x^2 \\rangle}+O\\left(\\frac{1}{N}\\right)\\right]} \\;,\n",
    "$$\n",
    "\n",
    "whose spread is controlled by the variance $\\langle \\Delta x^2\\rangle = 4pq N^{-1/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37aad7d-860f-45ac-a2dd-195c730f6082",
   "metadata": {},
   "source": [
    "```{note}\n",
    "One can show that, quite generally, large sums of random variables tend to Gaussians if correlations are short ranged. This is a consequence of the Central Limit Theorem (CLT). The Gaussian is completely fixed by knowing the first and second moment of the distribution. Since the first two moments can often be determined quite easily (see below), this is a great advantage for calculations. The scaling $\\langle \\Delta x^2\\rangle\\sim N^{-1/2}$ and the fact that the limiting distribution is independent of the higher order details of the distributions of individual random numbers is a first example of **universality**. See Section 1.5 of Arovas for a clear mathematical demonstration of the CLT.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8bafd-e80e-43a2-8144-e147cd11a08c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Calculation of first and second moment\n",
    "\n",
    "The $n^\\text{th}$ moment of a random variable X is defined by $\\langle X^n\\rangle$. The first and second moment of a distribution can often be computed analytically, and those are needed in the context of the CLT. Higher order moments are often more challenging to obtain.\n",
    "\n",
    "##### Expectation value\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\langle X\\rangle & =\\sum_{\\{\\sigma\\}} P(\\{\\sigma\\}) X(\\{\\sigma\\}) \\\\\n",
    "& =\\sum_{\\{\\sigma\\}}\\left[\\prod_{j} P\\left(\\sigma_{j}\\right)\\right]\\left[\\sum_{i} \\sigma_{i}\\right] \\\\\n",
    "& =\\sum_{j} \\sum_{\\sigma_{j}} P\\left(\\sigma_{j}\\right) \\cdot \\sigma_{j} \\\\\n",
    "& =\\sum_{j}(p-q)=N(p-q)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So, we see that the expectation value of X is an **extensive** quantity.\n",
    "\n",
    "Of course, a particular realization will (usually!) not have $X=\\langle X\\rangle$. \n",
    "\n",
    "One measure of spread is the\n",
    "\n",
    "##### Variance\n",
    "\n",
    "$$\n",
    "\\operatorname{var}(X) \\equiv\\langle\\underbrace{(X-\\langle X\\rangle)^{2}}_{\\equiv \\Delta X^2}\\rangle=\\left\\langle X^{2}\\right\\rangle-\\langle X\\rangle^{2} .\n",
    "$$\n",
    "\n",
    "To compute $\\left\\langle X^{2} \\right\\rangle$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\left\\langle X^{2}\\right\\rangle=\\sum_{\\{\\sigma\\}} P(\\{\\sigma\\}) X^{2}(\\{\\sigma\\}) \\\\\n",
    "& =\\sum_{\\{ \\sigma \\}} P(\\{\\sigma\\}) \\sum_{i, j} \\sigma_{i} \\cdot \\sigma_{j}=\\sum_{i, j}\\left\\langle\\sigma_{i} \\sigma_{j}\\right\\rangle \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "if $i=j: \\sigma_{i} \\cdot \\sigma_{i}=1 \\text {, so }\\left\\langle\\sigma_{i} \\cdot \\sigma_{i}\\right\\rangle=1$\n",
    "\n",
    "if $j\\neq j: \\left\\langle\\sigma_{i} \\sigma_{j}\\right\\rangle=\\left\\langle\\sigma_{i}\\right\\rangle\\left\\langle\\sigma_{i}\\right\\rangle=(p-q)^{2}$\n",
    "\n",
    "So, $\\left\\langle X^{2}\\right\\rangle=N+N(N-1)(p-q)^{2}$\n",
    "v. $\\langle X\\rangle^{2}=N^{2}(p-q)^{2}$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\langle X^{2}\\right\\rangle-\\langle X\\rangle^{2} & =N\\left(1-(p-q)^{2}\\right) . \\\\\n",
    "\\operatorname{var}(X) & =4 Npq .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note: Since $\\Delta X \\propto \\sqrt{N}$ but $\\langle X\\rangle\\propto N$, we have $\\frac{\\Delta x}{\\langle x\\rangle} \\sim N^{-1/2}\\rightarrow 0$ for $p \\neq q$ as $N\\to \\infty$.\n",
    "\n",
    "The $\\sqrt{N}$ scaling is a general consequence of the CLT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24034a5-653f-421e-83f1-6013a8cbc096",
   "metadata": {},
   "source": [
    "### Simulating coin-flips as random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef61ce-b585-4ff5-a9b6-c2adaa08689d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "S = 10000  # Number of samples (sequences)\n",
    "N = 200   # Length of each sequence\n",
    "Q = 100  # Number of sequences to plot\n",
    "\n",
    "# Parameters for the biased coin\n",
    "p = 0.3  # Probability of getting +1 (heads)\n",
    "q = 1 - p  # Probability of getting -1 (tails)\n",
    "\n",
    "# Generate S samples of N biased coin flips\n",
    "biased_coin_flips = np.random.choice([1, -1], (S, N), p=[p, q])\n",
    "\n",
    "# Plot 1: Sum of sigma_j from j=1 to k for Q of the S sequences (with biased coin)\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(Q):\n",
    "    plt.plot(np.cumsum(biased_coin_flips[i, :]), label=f'Sequence {i+1}')\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'Cumulative Sum $\\sum_{j=1}^k \\sigma_j$') #'Cumulative Sum of Sigma_j'\n",
    "plt.title(f'Cumulative Sum of Biased Coin Flips for Q={Q} Sequences (p={p})')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Histogram of the total sum of the sigmas of all sequences (with biased coin)\n",
    "biased_total_sums = np.sum(biased_coin_flips, axis=1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(biased_total_sums, bins=30, edgecolor='black')\n",
    "plt.xlabel('Total Sum of Sigmas')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Histogram of Total Sum of Sigmas for All Sequences (p={p})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a92450-d020-4180-842c-eeb0d8f6febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "# Theoretical expectation for the probability distribution of the sum of sigmas\n",
    "# The sum of S sigmas is equivalent to the number of heads minus the number of tails.\n",
    "# This can be modeled by a binomial distribution, where 'success' is getting a head (+1).\n",
    "# The total sum can range from -N (all tails) to +N (all heads).\n",
    "# The probability of k heads is binom.pmf(k, N, p), where k ranges from 0 to N.\n",
    "\n",
    "# Adjusting k to represent the sum of sigmas: k heads and N-k tails gives a sum of 2k-N.\n",
    "# The theoretical probabilities need to be scaled accordingly.\n",
    "theoretical_probs = [binom.pmf(k, N, p)/2 for k in range(N + 1)]\n",
    "scaled_sums = np.array([2 * k - N for k in range(N + 1)])\n",
    "\n",
    "# Plotting the histogram with the theoretical distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(biased_total_sums, bins=30, edgecolor='black', density=True, label='Empirical Distribution')\n",
    "plt.plot(scaled_sums, theoretical_probs, color='red', marker='o', linestyle='-', linewidth=2, label='Theoretical Distribution')\n",
    "plt.xlabel('Total Sum of Sigmas')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Histogram and Theoretical Distribution of Total Sum of Sigmas (p={p})')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15792484-e838-4d11-a157-63f924f80d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
